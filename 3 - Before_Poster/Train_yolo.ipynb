{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3UdWqYtu9KB_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.7.0)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (79.0.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexandre\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "#%pip install torch torchvision --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6sgaQXWTTY6T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onx4KjIYTd9Z"
      },
      "source": [
        "### test models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAHokhOfm5DM"
      },
      "source": [
        "from feedback:\n",
        "\n",
        "**For YOLOv8: **\n",
        "yolo detect train data=data.yaml model=yolov8n.pt\n",
        "imgsz=640 epochs=50 lr0=0.01 frozen=10.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkvekxjcnICd"
      },
      "source": [
        "a pre-trained YOLO model (you can choose n, s, m, l, or x versions) https://docs.ultralytics.com/models/yolov8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xx6Fdy3ThBo"
      },
      "source": [
        "---\n",
        "### * YOLOv8m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NvqQVCi0nTy0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m-seg.pt to 'yolov8m-seg.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 52.4M/52.4M [00:14<00:00, 3.80MB/s]\n"
          ]
        }
      ],
      "source": [
        "model_v8m = YOLO('yolov8m-seg.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z9CDnB9n-jDd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLOv8m-seg summary: 191 layers, 27,285,968 parameters, 0 gradients, 110.6 GFLOPs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(191, 27285968, 0, 110.64632320000001)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_v8m.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy7BcAyj9LvW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.145  Python-3.12.2 torch-2.7.0+cpu CPU (AMD Ryzen 5 3600 6-Core Processor)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./config.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=YOLOv8m-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=v8m_10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/segment, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\segment\\v8m_10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=11\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   5165393  ultralytics.nn.modules.head.Segment          [11, 32, 192, [192, 384, 576]]\n",
            "YOLOv8m-seg summary: 191 layers, 27,246,017 parameters, 27,246,001 gradients, 110.4 GFLOPs\n",
            "\n",
            "Transferred 531/537 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 7.23.4 MB/s, size: 82.3 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Alexandre\\Documents\\Visual_Studio_Code\\DL\\Nova pasta\\Car-damage-image-seg\\data\\train\\labels... 1395 images, 7 backgrounds, 0 corrupt: 100%|██████████| 1395/1395 [00:03<00:00, 403.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Alexandre\\Documents\\Visual_Studio_Code\\DL\\Nova pasta\\Car-damage-image-seg\\data\\train\\labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 5.81.4 MB/s, size: 84.5 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Alexandre\\Documents\\Visual_Studio_Code\\DL\\Nova pasta\\Car-damage-image-seg\\data\\valid\\labels... 401 images, 0 backgrounds, 0 corrupt: 100%|██████████| 401/401 [00:01<00:00, 379.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Alexandre\\Documents\\Visual_Studio_Code\\DL\\Nova pasta\\Car-damage-image-seg\\data\\valid\\labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "c:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs\\segment\\v8m_10\\labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 86 weight(decay=0.0), 97 weight(decay=0.0005), 96 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns\\segment\\v8m_10\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "       1/10         0G        1.6      3.613       3.98      1.906         37        640:  65%|██████▍   | 57/88 [28:21<15:37, 30.25s/it]"
          ]
        }
      ],
      "source": [
        "#treinar o modelo:\n",
        "#10 epochs:\n",
        "model_v8m = YOLO('YOLOv8m-seg.pt')\n",
        "model_v8m.train(data='./config.yaml', imgsz=640, epochs=10, lr0=0.01, freeze=10, project='runs/segment', name='v8m_10')\n",
        "metrics = model_v8m.val()\n",
        "print(metrics)\n",
        "#ate podemos testar 1 sem freeze para ver se a velocidade muda muito\n",
        "#model_v8m.train(data='./config.yaml', imgsz=640, epochs=15, lr0=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZRLsrf4vmYG"
      },
      "outputs": [],
      "source": [
        "#50 epochs:\n",
        "model_v8m = YOLO('YOLOv8m-seg.pt')\n",
        "model_v8m.train(data='./config.yaml', imgsz=640, epochs=25, lr0=0.01, freeze=10, project='runs/segment', name='v8m_25')\n",
        "metrics = model_v8m.val()\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA9PC_mkvuv3"
      },
      "outputs": [],
      "source": [
        "#100 epochs:\n",
        "model_v8m = YOLO('YOLOv8m-seg.pt')\n",
        "model_v8m.train(data='./config.yaml', imgsz=640, epochs=50, lr0=0.01, freeze=10, project='runs/segment', name='v8m_50')\n",
        "metrics = model_v8m.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE62cwx8qAh0"
      },
      "source": [
        "avaliar o modelo:\n",
        "https://www.ultralytics.com/glossary/mean-average-precision-map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuXKq63WlikD"
      },
      "source": [
        "* YOLOv8s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R29_r4qIDl5e"
      },
      "outputs": [],
      "source": [
        "model_v8s = YOLO('yolov8s-seg.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okaiDl1ppaFi"
      },
      "outputs": [],
      "source": [
        "#10 epochs:\n",
        "model_v8s = YOLO('yolov8s-seg.pt')\n",
        "model_v8s.train(data='./config.yaml', imgsz=640, epochs=10, lr0=0.01, freeze=10, project='runs/segment', name='v8s_10')\n",
        "metrics = model_v8s.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYY8L7yfv6Es"
      },
      "outputs": [],
      "source": [
        "\n",
        "#25 epochs:\n",
        "model_v8s = YOLO('yolov8s-seg.pt')\n",
        "model_v8s.train(data='./config.yaml', imgsz=640, epochs=25, lr0=0.01, freeze=10, project='runs/segment', name='v8s_25')\n",
        "metrics = model_v8s.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3kVfBDFwAlm"
      },
      "outputs": [],
      "source": [
        "#50 epochs:\n",
        "model_v8s = YOLO('yolov8s-seg.pt')\n",
        "model_v8s.train(data='./config.yaml', imgsz=640, epochs=50, lr0=0.01, freeze=10, project='runs/segment', name='v8s_50')\n",
        "metrics = model_v8s.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCA-lEExq0Ye"
      },
      "source": [
        "---\n",
        "#### Yolov11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6101FdrorBtG"
      },
      "source": [
        "YOLO model (you can choose n, s, m, l, or x versions) https://docs.ultralytics.com/tasks/segment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfQLaAdsravD"
      },
      "source": [
        "para o yolo 11 nao sei quanto de learning rate usar, ams comecemos com 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXXm-Py0rhAv"
      },
      "source": [
        "* yolo11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayv4_r2Vq5zU"
      },
      "outputs": [],
      "source": [
        "model_y11n = YOLO(\"yolo11n-seg.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eky_dZrWrZ3t"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y81X_TXMq-Cs"
      },
      "outputs": [],
      "source": [
        "#5 epochs:\n",
        "model_y11n = YOLO(\"yolo11n-seg.pt\")\n",
        "model_y11n.train(data='./config.yaml', imgsz=640, epochs=5, lr0=0.01, freeze=10, project='runs/segment', name='y11n_5')\n",
        "metrics = model_y11n.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPzUdlSBwUUV"
      },
      "outputs": [],
      "source": [
        "#15 epochs\n",
        "model_y11n = YOLO(\"yolo11n-seg.pt\")\n",
        "model_y11n.train(data='./config.yaml', imgsz=640, epochs=15, lr0=0.01, freeze=10, project='runs/segment', name='y11n_15')\n",
        "metrics = model_y11n.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_BlAHbQsbbk"
      },
      "outputs": [],
      "source": [
        "#30 epochs:\n",
        "model_y11n = YOLO(\"yolo11n-seg.pt\")\n",
        "model_y11n.train(data='./config.yaml', imgsz=640, epochs=30, lr0=0.01, freeze=10, project='runs/segment', name='y11n_30')\n",
        "metrics = model_y11n.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYzNfmb9t901"
      },
      "source": [
        "* yolo11l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqLD5AJjt_8M"
      },
      "outputs": [],
      "source": [
        "model_y11l = YOLO(\"yolo11l-seg.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPZ-NrFnuOtZ"
      },
      "outputs": [],
      "source": [
        "#5 epochs:\n",
        "model_y11l = YOLO(\"yolo11l-seg.pt\")\n",
        "model_y11l.train(data='./config.yaml', imgsz=640, epochs=5, lr0=0.01, freeze=10, project='runs/segment', name='y11l_5')\n",
        "metrics = model_y11l.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghaD47WawmnN"
      },
      "outputs": [],
      "source": [
        "#15 epochs:\n",
        "model_y11l = YOLO(\"yolo11l-seg.pt\")\n",
        "model_y11l.train(data='./config.yaml', imgsz=640, epochs=15, lr0=0.01, freeze=10, project='runs/segment', name='y11l_15')\n",
        "metrics = model_y11l.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KAEuAD7uOsc"
      },
      "outputs": [],
      "source": [
        "#30 epochs:\n",
        "model_y11l = YOLO(\"yolo11l-seg.pt\")\n",
        "model_y11l.train(data='./config.yaml', imgsz=640, epochs=30, lr0=0.01, freeze=10, project='runs/segment', name='y11l_30')\n",
        "metrics = model_y11l.val()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B2Xg_gsmOMt"
      },
      "source": [
        "---\n",
        "#### Prediction:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL4V-Hrhpq98"
      },
      "source": [
        "a prediction so vamos usar depois de escolhermos , ou entao podemos fazer para comparar modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut_dVQ0gDvq_"
      },
      "outputs": [],
      "source": [
        "#aqui tens que ver qual o ficheiro em que fica guardado os saves do modelo\n",
        "#also, esta o last mas tambem podemos testar com o 'best'\n",
        "model = YOLO('./runs/segment/train4/weights/last.pt')\n",
        "\n",
        "results = model('./car_1.jpg', imgsz=640)\n",
        "\n",
        "annotated = results[0].plot()\n",
        "cv2.imwrite('car_1_overlay.png', annotated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WC8yJrzIIMw",
        "outputId": "2852fa2b-e5f4-4e61-bd4a-e46d967a8b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#neste, alterei o nivel de confiança para aparecer mais ou menos labels:\n",
        "results = model.predict(source='./car_1.jpg', conf=0.25, show=True)\n",
        "annotated = results[0].plot()\n",
        "\n",
        "cv2.imwrite('car_1_overlay_conf.png', annotated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f_mq78rZr8n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm3rA1BaZA4h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
